{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Retraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# wandb - hyperparameter sweep and Train monitoring\n",
    "import wandb\n",
    "#torch - computing and machine learning libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "# seisbench\n",
    "import seisbench.models as sbm\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# seisynth\n",
    "from utils.common import load_dataset_and_labels, load_pretrained_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Possible values\n",
    "DATASETS_ORIGINS = ['ethz', 'geofon']\n",
    "SBM_CLASSES= [sbm.PhaseNet, sbm.EQTransformer]\n",
    "MODEL_TO_NUM_SAMPLES = {sbm.EQTransformer:6000, sbm.PhaseNet: 3001}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_origin = 'geofon'\n",
    "assert dataset_origin in DATASETS_ORIGINS, f'Expected dataset one of {DATASETS_ORIGINS}. Got {dataset_origin}.'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "seisbench.models.eqtransformer.EQTransformer"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SBM_CLASS= sbm.EQTransformer\n",
    "assert SBM_CLASS in SBM_CLASSES\n",
    "SBM_CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "6000"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES=MODEL_TO_NUM_SAMPLES[SBM_CLASS]\n",
    "NUM_SAMPLES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "NUM_SHIFTS=6\n",
    "SAMPLE_RATE=100\n",
    "LARGE_ERROR_THRESHOLD_SECONDS=1\n",
    "LARGE_ERROR_THRESHOLD_SAMPLES=LARGE_ERROR_THRESHOLD_SECONDS*SAMPLE_RATE\n",
    "SYNTHESIZED_SNR=10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "('/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH=f'/home/moshe/datasets/GFZ/noisy_datasets/{dataset_origin}_{NUM_SAMPLES}_sample_joachim_noises_energy_ratio_snr/'\n",
    "NOISY_DATA_PATH = os.path.join(DATASET_PATH, f'noisy_dataset_snr_{SYNTHESIZED_SNR}')\n",
    "DATASET_PATH, NOISY_DATA_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def assert_path_exists(path_str: str, name: str=''):\n",
    "    assert os.path.exists(path_str), f'{name} {path_str} does not exist'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "assert_path_exists(path_str=DATASET_PATH, name='DATASET_PATH')\n",
    "assert_path_exists(path_str=NOISY_DATA_PATH, name='NOISY_DATA_PATH')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Pretrained Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model with the pretrained weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with <class 'seisbench.models.eqtransformer.EQTransformer'> on GEOFON\n",
      "Load <class 'seisbench.models.eqtransformer.EQTransformer'> pretrained weights\n",
      "<class 'seisbench.models.eqtransformer.EQTransformer'> pretrained keys ['ethz', 'geofon', 'instance', 'iquique', 'lendb', 'neic', 'obs', 'original', 'original_nonconservative', 'scedc', 'stead']\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save a copy for retraining. One model will be trained and the other one will keep the current weights for benchmarking on specific examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with <class 'seisbench.models.eqtransformer.EQTransformer'> on GEOFON\n",
      "Load <class 'seisbench.models.eqtransformer.EQTransformer'> pretrained weights\n",
      "<class 'seisbench.models.eqtransformer.EQTransformer'> pretrained keys ['ethz', 'geofon', 'instance', 'iquique', 'lendb', 'neic', 'obs', 'original', 'original_nonconservative', 'scedc', 'stead']\n"
     ]
    }
   ],
   "source": [
    "# reloading because I cannot torch clone. Seisbench models are not nn.Module :(\n",
    "retraining_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "EQTransformer(\n  (encoder): Encoder(\n    (convs): ModuleList(\n      (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      (1): Conv1d(8, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (3): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (5): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n    )\n    (pools): ModuleList(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (res_cnn_stack): ResCNNStack(\n    (members): ModuleList(\n      (0): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (1): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (2): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (3): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (4): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n      (5): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (6): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n    )\n  )\n  (bi_lstm_stack): BiLSTMStack(\n    (members): ModuleList(\n      (0): BiLSTMBlock(\n        (lstm): LSTM(64, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BiLSTMBlock(\n        (lstm): LSTM(16, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BiLSTMBlock(\n        (lstm): LSTM(16, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (transformer_d0): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (transformer_d): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (decoder_d): Decoder(\n    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n    (convs): ModuleList(\n      (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n    )\n  )\n  (conv_d): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  (dropout): Dropout(p=0.1, inplace=False)\n  (pick_lstms): ModuleList(\n    (0): LSTM(16, 16)\n    (1): LSTM(16, 16)\n  )\n  (pick_attentions): ModuleList(\n    (0): SeqSelfAttention()\n    (1): SeqSelfAttention()\n  )\n  (pick_decoders): ModuleList(\n    (0): Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n    (1): Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n  )\n  (pick_convs): ModuleList(\n    (0): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n    (1): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  )\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.eval()\n",
    "retraining_model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Datasets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have 4 datasets:\n",
    "1. original_dataset - ETHZ/GEOFON original traces filtered to have high estimated SNR - more than 20dB\n",
    "2. le_original_dataset - A subset of the original_dataset (high SNR traces) that the pretrained model had a large picking error.\n",
    "3. noised_dataset - Traces taken from the original dataset and merged with noise traces such that the resulting trace is a 10 dB SNR trace.\n",
    "4. le_noised_dataset -  A subset of the noised_dataset that the pretrained model had a large picking error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "original_dataset_path = os.path.join(DATASET_PATH, 'original_dataset.pt')\n",
    "assert_path_exists(path_str=original_dataset_path)\n",
    "original_labels_path = os.path.join(DATASET_PATH, 'original_labels.pt')\n",
    "assert_path_exists(path_str=original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "original_dataset, original_labels = load_dataset_and_labels(dataset_path=original_dataset_path, labels_path=original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11146 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {original_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "le_original_dataset_path = os.path.join(DATASET_PATH, 'le_original_dataset.pt')\n",
    "assert_path_exists(path_str=le_original_dataset_path)\n",
    "le_original_labels_path = os.path.join(DATASET_PATH, 'le_original_labels.pt')\n",
    "assert_path_exists(path_str=le_original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "le_original_dataset, le_original_labels = load_dataset_and_labels(dataset_path=le_original_dataset_path, labels_path=le_original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 297 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {le_original_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "('/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/labels.pt')"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_dataset_path= os.path.join(NOISY_DATA_PATH, 'traces.pt')\n",
    "assert_path_exists(path_str=noised_dataset_path)\n",
    "noised_labels_path= os.path.join(NOISY_DATA_PATH, 'labels.pt')\n",
    "assert_path_exists(path_str=noised_labels_path)\n",
    "noised_dataset_path, noised_labels_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "noised_dataset, noised_labels = load_dataset_and_labels(dataset_path=noised_dataset_path, labels_path=noised_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {noised_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(\"/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/le_<class 'seisbench.models.eqtransformer.EQTransformer'>_dataset.pt\",\n \"/home/moshe/datasets/GFZ/noisy_datasets/geofon_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/le_<class 'seisbench.models.eqtransformer.EQTransformer'>_labels.pt\")"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_noised_dataset_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_dataset.pt')\n",
    "assert_path_exists(path_str=le_noised_dataset_path)\n",
    "le_noised_labels_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_labels.pt')\n",
    "assert_path_exists(path_str=le_noised_labels_path)\n",
    "le_noised_dataset_path, le_noised_labels_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "le_noised_dataset, le_noised_labels = load_dataset_and_labels(dataset_path=le_noised_dataset_path, labels_path=le_noised_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1687 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {le_noised_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrange Train\\Validation\\Test Sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_dataset_inds, val_dataset_inds, test_dataset_inds = random_split(range(noised_dataset.shape[0]), [0.8,0.1,0.1], generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = noised_dataset[train_dataset_inds], noised_dataset[val_dataset_inds], noised_dataset[test_dataset_inds]\n",
    "train_labels, val_labels, test_labels = noised_labels[train_dataset_inds], noised_labels[val_dataset_inds], noised_labels[test_dataset_inds]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train set with 6400 traces, validation set with 800 traces and test set with 800 traces.\n"
     ]
    }
   ],
   "source": [
    "print(f'Created train set with {train_dataset.shape[0]} traces, validation set with {val_dataset.shape[0]} traces and test set with {test_dataset.shape[0]} traces.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Custom Datasets/DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def label_normal_smooth(label):\n",
    "    num_samples = NUM_SAMPLES\n",
    "    sigma = 1000.0\n",
    "    v = torch.arange(num_samples).double()\n",
    "    return (1.0/(sigma*torch.sqrt(2.0* torch.tensor(torch.pi)))) * torch.exp(-0.5*torch.square((v-label)/sigma))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset: torch.tensor, labels: torch.tensor, transform=None, target_transform=None):\n",
    "        self._dataset = dataset\n",
    "        self._labels = labels\n",
    "        assert dataset.dim() == 3, f'Expected 3 dim dataset tensor (#traces,#channels,#samples). Got {dataset.dim()} dims. Shape {dataset.shape} '\n",
    "        assert labels.shape[0] == dataset.shape[0], f'Expected 1 label per trace. Got {labels.shape[0]} for {dataset.shape[0]} traces'\n",
    "        self._len = int(dataset.shape[0])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trace = self._dataset[idx]\n",
    "        label = self._labels[idx]\n",
    "        if self.transform:\n",
    "            trace = self.transform(trace)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return trace, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "trainset = CustomDataset(dataset=train_dataset, labels=train_labels, target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "valset = CustomDataset(dataset=val_dataset, labels=val_labels, target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "testset = CustomDataset(dataset=test_dataset, labels=test_labels, target_transform=lambda l: (l,label_normal_smooth(l)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train set with 6400 traces, validation set with 800 traces and test set with 800 traces.\n"
     ]
    }
   ],
   "source": [
    "print(f'Created train set with {len(trainset)} traces, validation set with {len(valset)} traces and test set with {len(testset)} traces.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taken from Seisbench tutorial notebook \"03a_training_phasenet\"  -  not using it for now - commented out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true, eps=1e-5):\n",
    "    # vector cross entropy loss\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "\n",
    "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
    "\n",
    "    h = h.mean()  # Mean over batch axis\n",
    "\n",
    "    return -h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, large_error_threshold=100):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0.0\n",
    "    large_errors_counter = 0\n",
    "    mean_residual = 0.0\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        trace, (label, label_smoothed) = batch\n",
    "\n",
    "        # print('trace shape', trace.shape, 'label shape', label.shape, 'label_smoothed shape', label_smoothed.shape)\n",
    "\n",
    "        batch_size = trace.shape[0]\n",
    "        # print('batch_size', batch_size)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        # Fwd pass - outputs the likelihood function\n",
    "        pred_probs = model(trace.to(model.device))\n",
    "\n",
    "        if SBM_CLASS == sbm.EQTransformer:\n",
    "            # EQTransformer returns a tuple (N,Z,E)\n",
    "            pred_probs = torch.stack((pred_probs[1],pred_probs[0],pred_probs[2]), dim=0).swapaxes(0,1)\n",
    "        # print('pred_probs shape', pred_probs.shape)\n",
    "\n",
    "\n",
    "        # softargmax\n",
    "        beta = 100.0\n",
    "        softmax = torch.nn.functional.softmax(beta  * pred_probs[:, 0, :], dim=-1)\n",
    "        indices = torch.arange(pred_probs[:, 0, :].shape[-1])\n",
    "        softargmax_preds = torch.sum(torch.mul(indices, softmax), dim=-1)\n",
    "\n",
    "        loss = torch.abs(softargmax_preds - label).mean()\n",
    "\n",
    "        # loss = loss_fn(pred_probs[:,0,:], label_smoothed.double().to(model.device))\n",
    "        # print('loss', loss)\n",
    "        # loss = loss_fn(F.log_softmax(pred_probs[:,0,:], dim=-1), F.log_softmax(label.to(model.device), dim=-1))\n",
    "\n",
    "        prediction = torch.argmax(pred_probs[:, 0, :], dim=-1)\n",
    "        # print('prediction', prediction.shape)\n",
    "        residual = torch.abs(prediction - label.to(model.device))\n",
    "        # print(residual)\n",
    "        mean_residual += float(residual.mean())\n",
    "        # print('mean_residual', mean_residual)\n",
    "        # print(residual > large_error_threshold)\n",
    "        # print(residual[residual > large_error_threshold])\n",
    "        # print((residual > large_error_threshold).sum())\n",
    "        # large_errors_counter += (1 if residual > large_error_threshold else 0)\n",
    "        large_errors_counter += int((residual > large_error_threshold).sum())\n",
    "        # print(large_errors_counter)\n",
    "\n",
    "        # if batch_id == 2:\n",
    "        #     break\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # if batch_id % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch_id * trace.shape[0]\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    mean_residual /= num_batches\n",
    "    train_loss /= num_batches\n",
    "\n",
    "    # print(mean_residual)\n",
    "    # print(train_loss)\n",
    "    # raise Exception\n",
    "    return train_loss, mean_residual, large_errors_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(dataloader, model, loss_fn, large_error_threshold=100):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    large_errors_counter = 0\n",
    "    mean_residual = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            trace, (label, label_smoothed) = batch\n",
    "            pred_probs = model(trace.to(model.device))\n",
    "            if SBM_CLASS == sbm.EQTransformer:\n",
    "                # EQTransformer returns a tuple (N,Z,E)\n",
    "                pred_probs = torch.stack((pred_probs[1],pred_probs[0],pred_probs[2]), dim=0).swapaxes(0,1)\n",
    "            # Take the maximum of the z channel prediction\n",
    "            loss = loss_fn(pred_probs[:,0,:], label_smoothed.to(model.device))\n",
    "            # loss = loss_fn(F.log_softmax(pred_probs[:,0,:], dim=-1), F.log_softmax(label_smoothed.to(model.device), dim=-1))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            prediction = torch.argmax(pred_probs[:, 0, :], dim=-1)\n",
    "            residual = float(torch.abs(prediction - label.to(model.device)))\n",
    "            mean_residual += residual\n",
    "            large_errors_counter += (1 if residual > large_error_threshold else 0)\n",
    "\n",
    "    mean_residual /= num_batches\n",
    "    test_loss /= num_batches\n",
    "    return test_loss, mean_residual, large_errors_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def train(trainset, valset, trained_model, benchmark_model, epochs, learning_rate, batch_size):\n",
    "    # init weights&biases monitoring\n",
    "    wandb.init(project=\"seisynth\", entity=\"moshebeutel\")\n",
    "    wandb.config = {\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": 1}\n",
    "\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = DataLoader(valset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Define the train optimizer and optimization criterion\n",
    "    # optimizer = torch.optim.SGD(trained_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-3)\n",
    "    optimizer = torch.optim.Adam(trained_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # criterion = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "    # criterion = torch.nn.MSELoss()\n",
    "\n",
    "    criterion = loss_fn\n",
    "\n",
    "    # Evaluate the benchmark model on the validation data.\n",
    "    # The benchmark model is not training so it is done once.\n",
    "    benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter =  test_loop(dataloader=val_dataloader, model=benchmark_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "    print(f'Benchmark results: loss {benchmark_loss}, Mean Residual {benchmark_mean_residual}, Errors Above {LARGE_ERROR_THRESHOLD_SECONDS} sec. {benchmark_large_errors_counter}')\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for t in pbar:\n",
    "        epoch_train_loss, epoch_train_mean_residual, epoch_train_large_errors_counter = train_loop(dataloader=train_dataloader, model=trained_model, loss_fn=criterion , optimizer=optimizer, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "        epoch_val_loss, epoch_val_mean_residual, epoch_val_large_errors_counter = test_loop(dataloader=val_dataloader, model=trained_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "\n",
    "        wandb.log({'epoch train loss': epoch_train_loss,\n",
    "                   'epoch_train_mean_residual':epoch_train_mean_residual,\n",
    "                   'epoch_train_large_errors_counter':epoch_train_large_errors_counter,\n",
    "                   'epoch validation loss': epoch_val_loss,\n",
    "                   'epoch_val_mean_residual':epoch_val_mean_residual,\n",
    "                   'epoch_val_large_errors_counter':epoch_val_large_errors_counter})\n",
    "\n",
    "        pbar.set_description(f'Epoch {t}, train loss {epoch_train_loss}, validation loss {epoch_val_loss}, epoch_val_large_errors_counter {epoch_val_large_errors_counter}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "#\n",
    "# def normal_smooth(label):\n",
    "#     num_samples = 3001\n",
    "#     sigma = 1000\n",
    "#     v = torch.arange(num_samples).float()\n",
    "#     return (1.0/(sigma*torch.sqrt(2.0* torch.tensor(torch.pi)))) * torch.exp(-0.5*torch.square((v-label)/sigma))\n",
    "#\n",
    "# a = normal_smooth(1000) * 1e6\n",
    "# b = normal_smooth(1200) * 1e6\n",
    "#\n",
    "# fig, (ax_a, ax_b) = plt.subplots(1,2, sharey='all');\n",
    "# ax_a.plot(a);\n",
    "# ax_b.plot(b);\n",
    "#\n",
    "# torch.nn.MSELoss()(a,b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "LEARNING_RATE = 1e-6\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Call Train Entry Point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(testset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmoshebeutel\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/moshe/GIT/summer_2022_Seismology/notebooks/wandb/run-20230112_133121-h68awa2m</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/moshebeutel/seisynth/runs/h68awa2m\" target=\"_blank\">bright-terrain-53</a></strong> to <a href=\"https://wandb.ai/moshebeutel/seisynth\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/moshebeutel/seisynth\" target=\"_blank\">https://wandb.ai/moshebeutel/seisynth</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/moshebeutel/seisynth/runs/h68awa2m\" target=\"_blank\">https://wandb.ai/moshebeutel/seisynth/runs/h68awa2m</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results: loss 0.0013749178744094418, Mean Residual 92.25378262500013, Errors Above 1 sec. 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, train loss 90.38634143490681, validation loss 0.0014182093281871258, epoch_val_large_errors_counter 138:   5%|‚ñç         | 14/300 [1:23:01<28:26:55, 358.10s/it]"
     ]
    }
   ],
   "source": [
    "train(trainset=trainset, valset=valset, trained_model=retraining_model.double(), benchmark_model=pretrained_model.double(), epochs=EPOCHS, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark_dataloader = DataLoader(valset, batch_size=1, shuffle=False)\n",
    "# train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "criterion = loss_fn\n",
    "benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter =  test_loop(dataloader=benchmark_dataloader, model=pretrained_model.double(), loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter, float(benchmark_large_errors_counter) / float(len(benchmark_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter, float(benchmark_large_errors_counter) / float(len(benchmark_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# softargmax_demo.py\n",
    "\n",
    "# import torch\n",
    "#\n",
    "# def softargmax(x):\n",
    "#   # crude: assumes max value is unique\n",
    "#   beta = 100.0\n",
    "#   xx = beta  * x\n",
    "#   sm = torch.nn.functional.softmax(xx, dim=-1)\n",
    "#   indices = torch.arange(x.shape[-1])\n",
    "#   y = torch.mul(indices, sm)\n",
    "#   result = torch.sum(y, dim=-1)\n",
    "#   return result\n",
    "#\n",
    "# print(\"\\nBegin PyTorch softargmax demo \")\n",
    "#\n",
    "# t = torch.randint(low=1, high=100, size=(2,10))\n",
    "# print(\"\\nSource tensor: \")\n",
    "# print(t)\n",
    "#\n",
    "# sam = softargmax(t)\n",
    "# print(\"\\nValue of softargmax(): \")\n",
    "# print(sam)\n",
    "#\n",
    "# print(\"\\nEnd \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
